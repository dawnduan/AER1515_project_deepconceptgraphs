{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Setup"
      ],
      "metadata": {
        "id": "x6gOGtW0mEwm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --no-deps -q bitsandbytes\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "axaHlCqp8vMO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "477e0321-e18b-49ff-8ae2-d2a2ee8ca98d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hMounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Ca1UlkaBnySQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install addict transformers==4.46.3 tokenizers==0.20.3 supervision open-clip-torch"
      ],
      "metadata": {
        "id": "lRq1Gp3R5veF",
        "outputId": "8ce25b24-aa95-45e5-8453-b5f8c604814a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting addict\n",
            "  Downloading addict-2.4.0-py3-none-any.whl.metadata (1.0 kB)\n",
            "Collecting transformers==4.46.3\n",
            "  Downloading transformers-4.46.3-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers==0.20.3\n",
            "  Downloading tokenizers-0.20.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting supervision\n",
            "  Downloading supervision-0.27.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting open-clip-torch\n",
            "  Downloading open_clip_torch-3.2.0-py3-none-any.whl.metadata (32 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers==4.46.3) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.12/dist-packages (from transformers==4.46.3) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.46.3) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.46.3) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.46.3) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.46.3) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers==4.46.3) (2.32.4)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.46.3) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers==4.46.3) (4.67.1)\n",
            "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from supervision) (1.16.3)\n",
            "Requirement already satisfied: matplotlib>=3.6.0 in /usr/local/lib/python3.12/dist-packages (from supervision) (3.10.0)\n",
            "Requirement already satisfied: defusedxml>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from supervision) (0.7.1)\n",
            "Requirement already satisfied: pillow>=9.4 in /usr/local/lib/python3.12/dist-packages (from supervision) (11.3.0)\n",
            "Requirement already satisfied: opencv-python>=4.5.5.64 in /usr/local/lib/python3.12/dist-packages (from supervision) (4.12.0.88)\n",
            "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.12/dist-packages (from open-clip-torch) (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from open-clip-torch) (0.24.0+cu126)\n",
            "Collecting ftfy (from open-clip-torch)\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: timm>=1.0.17 in /usr/local/lib/python3.12/dist-packages (from open-clip-torch) (1.0.22)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.46.3) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.46.3) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.46.3) (1.2.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.6.0->supervision) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.6.0->supervision) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.6.0->supervision) (4.61.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.6.0->supervision) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.6.0->supervision) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.6.0->supervision) (2.9.0.post0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.46.3) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.46.3) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.46.3) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.46.3) (2025.11.12)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (3.5.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from ftfy->open-clip-torch) (0.2.14)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.6.0->supervision) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0->open-clip-torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0->open-clip-torch) (3.0.3)\n",
            "Downloading transformers-4.46.3-py3-none-any.whl (10.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m100.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.20.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m117.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n",
            "Downloading supervision-0.27.0-py3-none-any.whl (212 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.4/212.4 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading open_clip_torch-3.2.0-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m86.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: addict, ftfy, tokenizers, supervision, transformers, open-clip-torch\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.22.1\n",
            "    Uninstalling tokenizers-0.22.1:\n",
            "      Successfully uninstalled tokenizers-0.22.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.57.3\n",
            "    Uninstalling transformers-4.57.3:\n",
            "      Successfully uninstalled transformers-4.57.3\n",
            "Successfully installed addict-2.4.0 ftfy-6.3.1 open-clip-torch-3.2.0 supervision-0.27.0 tokenizers-0.20.3 transformers-4.46.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### OCR"
      ],
      "metadata": {
        "id": "X5ETqD2OmMHM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "4urzCKTC5ArJ"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModel, AutoTokenizer, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "model_name = 'deepseek-ai/DeepSeek-OCR'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQd7KllGlAoU",
        "outputId": "d3ed586c-2227-4fba-9eb8-2ef1435d0694"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zoc_AWCglZIA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Scene Graph Pipeline: Encoder (Object Detection) + Decoder (Relationship Generation)\n",
        "Processes images to detect objects, then generates relationships between them.\n",
        "All logs saved to timestamped folder: log_YYMMDDHH\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "import itertools\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from transformers import AutoModel, AutoTokenizer, BitsAndBytesConfig\n",
        "from PIL import Image\n",
        "import sys\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# CONFIGURATION\n",
        "# ============================================================================\n",
        "\n",
        "# Input/Output Paths\n",
        "os.chdir('/content/drive/MyDrive/AER1515_Assignment1/aer1515_project/')\n",
        "IMAGE_INPUT_DIR = \"./frame00180_test\"  # Directory containing input images (jpg/png)\n",
        "OUTPUT_BASE_DIR = \"./frame00180_test/logs\"  # Base directory for logs\n",
        "\n",
        "# Model Configuration\n",
        "MODEL_NAME = 'deepseek-ai/DeepSeek-OCR'\n",
        "\n",
        "# Generation Parameters\n",
        "MAX_NEW_TOKENS = 64\n",
        "TEMPERATURE = 0.0\n",
        "\n",
        "# Relationship Keywords for Extraction\n",
        "RELATION_KEYWORDS = [\n",
        "    \"on top of\", \"under\", \"next to\", \"beside\", \"in front of\",\n",
        "    \"behind\", \"inside\", \"above\", \"below\", \"around\",\n",
        "    \"attached to\", \"leaning against\"\n",
        "]\n",
        "\n",
        "# ============================================================================\n",
        "# SETUP LOGGING DIRECTORY\n",
        "# ============================================================================\n",
        "\n",
        "def setup_log_directory():\n",
        "    \"\"\"Create timestamped log directory.\"\"\"\n",
        "    timestamp = datetime.now().strftime(\"%y%m%d%H\")\n",
        "    log_dir = Path(OUTPUT_BASE_DIR) / f\"log_{timestamp}\"\n",
        "    log_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Create subdirectories\n",
        "    (log_dir / \"encoder_outputs\").mkdir(exist_ok=True)\n",
        "    (log_dir / \"decoder_outputs\").mkdir(exist_ok=True)\n",
        "    (log_dir / \"visualizations\").mkdir(exist_ok=True)\n",
        "\n",
        "    return log_dir\n",
        "\n",
        "# ============================================================================\n",
        "# ENCODER: OBJECT DETECTION WITH BOUNDING BOXES\n",
        "# ============================================================================\n",
        "import re\n",
        "import ast\n",
        "import json\n",
        "\n",
        "def detections_to_json(res: str):\n",
        "    \"\"\"\n",
        "    Parse strings like:\n",
        "      <|ref|>Cabinet<|/ref|><|det|>[[472, 0, 810, 380]]<|/det|>\n",
        "    into a dict:\n",
        "      {\n",
        "        \"object_0\": {\"id\": 0, \"object_tag\": \"Cabinet\", \"bbox\": [472, 0, 810, 380]},\n",
        "        ...\n",
        "      }\n",
        "    \"\"\"\n",
        "    pattern = r\"<\\|ref\\|>(.*?)<\\|/ref\\|><\\|det\\|>(\\[\\[.*?\\]\\])<\\|/det\\|>\"\n",
        "    matches = re.findall(pattern, res, flags=re.DOTALL)\n",
        "\n",
        "    data = {}\n",
        "    for i, (name, bbox_str) in enumerate(matches):\n",
        "        try:\n",
        "            # Expecting something like [[x1, y1, x2, y2]]\n",
        "            bbox = ast.literal_eval(bbox_str)[0]\n",
        "        except (SyntaxError, ValueError, IndexError):\n",
        "            # Skip malformed entries\n",
        "            continue\n",
        "\n",
        "        data[f\"object_{i}\"] = {\n",
        "            \"id\": i,\n",
        "            \"object_tag\": name.lower(),\n",
        "            \"bbox\": bbox,\n",
        "            # dummy 3D info, you can add:\n",
        "            \"bbox_extent\": [],\n",
        "            \"bbox_center\": [],\n",
        "            \"bbox_volume\": -1,\n",
        "        }\n",
        "        # print(data)\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "\n",
        "class ObjectEncoder:\n",
        "    \"\"\"Detects objects in images and outputs bounding boxes.\"\"\"\n",
        "\n",
        "    def __init__(self, model_name, log_dir):\n",
        "        self.log_dir = log_dir\n",
        "        self.log_file = open(log_dir / \"encoder_log.txt\", \"w\")\n",
        "        self.log(\"Initializing Object Encoder...\")\n",
        "\n",
        "        # Setup quantization config\n",
        "        qc = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_compute_dtype=torch.float\n",
        "        )\n",
        "\n",
        "        # Load model and tokenizer\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
        "            model_name, trust_remote_code=True\n",
        "        )\n",
        "        self.model = AutoModel.from_pretrained(\n",
        "            model_name,\n",
        "            trust_remote_code=True,\n",
        "            use_safetensors=True,\n",
        "            device_map=\"auto\",\n",
        "            quantization_config=qc,\n",
        "            torch_dtype=torch.float\n",
        "        )\n",
        "        self.model = self.model.eval()\n",
        "        self.log(\"Encoder model loaded successfully!\")\n",
        "\n",
        "    def log(self, message):\n",
        "        \"\"\"Log message to both console and file.\"\"\"\n",
        "        print(f\"[ENCODER] {message}\")\n",
        "        self.log_file.write(f\"{message}\\n\")\n",
        "        self.log_file.flush()\n",
        "\n",
        "    def process_image(self, image_path, output_name):\n",
        "        \"\"\"Process single image to detect objects with bounding boxes.\"\"\"\n",
        "        self.log(f\"\\nProcessing: {image_path}\")\n",
        "\n",
        "        prompt = \"<image>\\nIdentify all objects in the image and output them in bounding boxes.\"\n",
        "\n",
        "        output_dir = self.log_dir / \"encoder_outputs\" / output_name\n",
        "        output_dir.mkdir(exist_ok=True)\n",
        "\n",
        "        try:\n",
        "            log_buffer = StringIO()\n",
        "            with contextlib.redirect_stdout(log_buffer), contextlib.redirect_stderr(log_buffer):\n",
        "              with torch.no_grad():\n",
        "                  res = self.model.infer(\n",
        "                      self.tokenizer,\n",
        "                      prompt=prompt,\n",
        "                      image_file=str(image_path),\n",
        "                      output_path=str(output_dir),\n",
        "                      base_size=1024,\n",
        "                      image_size=1024,\n",
        "                      crop_mode=False,\n",
        "                      save_results=True,\n",
        "                      test_compress=False,\n",
        "                      eval_mode=False\n",
        "                  )\n",
        "\n",
        "            self.log(f\"✓ Successfully processed {image_path.name}\")\n",
        "            log_text = log_buffer.getvalue()\n",
        "            # Save result text\n",
        "            result_file = output_dir / \"detection_result.txt\"\n",
        "            with open(result_file, \"w\") as f:\n",
        "                f.write(log_text)\n",
        "            parsed = detections_to_json(log_text)\n",
        "            detected_obj = detections_to_json(log_text)\n",
        "\n",
        "            result_json_file = output_dir / \"detection_result.json\"\n",
        "            with open(result_json_file, \"w\") as f:\n",
        "                json.dump(detections_to_json(log_text), f, indent=2)\n",
        "\n",
        "            return res, output_dir, detections_to_json(log_text)\n",
        "\n",
        "        except Exception as e:\n",
        "            self.log(f\"✗ Error processing {image_path.name}: {str(e)}\")\n",
        "            return None, None, {}\n",
        "\n",
        "    def process_directory(self, image_dir):\n",
        "        \"\"\"Process all images in directory.\"\"\"\n",
        "        image_dir = Path(image_dir)\n",
        "        image_files = list(image_dir.glob(\"*.jpg\")) + list(image_dir.glob(\"*.png\"))\n",
        "\n",
        "        self.log(f\"\\nFound {len(image_files)} images to process\")\n",
        "\n",
        "        results = {}\n",
        "        object_col = {}\n",
        "        for img_file in image_files:\n",
        "            output_name = img_file.stem\n",
        "            res, output_dir, data = self.process_image(img_file, output_name)\n",
        "            if res is not None:\n",
        "                object_col.update(data)\n",
        "                results[output_name] = {\n",
        "                    \"result\": res,\n",
        "                    \"output_dir\": output_dir\n",
        "                }\n",
        "\n",
        "        result_json_file =  image_dir/ \"object_result.json\"\n",
        "        with open(result_json_file, \"w\") as f:\n",
        "          json.dump(parsed, f, indent=2)\n",
        "        self.log(f\"\\n✓ Encoder complete: {len(results)}/{len(image_files)} images processed\")\n",
        "        return results\n",
        "\n",
        "    def close(self):\n",
        "        \"\"\"Close log file.\"\"\"\n",
        "        self.log_file.close()\n",
        "\n",
        "# ============================================================================\n",
        "# DECODER: RELATIONSHIP GENERATION\n",
        "# ============================================================================\n",
        "import contextlib\n",
        "from io import StringIO\n",
        "class RelationshipDecoder:\n",
        "    \"\"\"Generates relationships between detected objects.\"\"\"\n",
        "\n",
        "    def __init__(self, model_name, log_dir):\n",
        "        self.log_dir = log_dir\n",
        "        self.log_file = open(log_dir / \"decoder_log.txt\", \"w\")\n",
        "        self.log(\"Initializing Relationship Decoder...\")\n",
        "\n",
        "        # Setup quantization config\n",
        "        qc = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_compute_dtype=torch.float\n",
        "        )\n",
        "\n",
        "        # Load model and tokenizer\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
        "            model_name, trust_remote_code=True\n",
        "        )\n",
        "        self.model = AutoModel.from_pretrained(\n",
        "            model_name,\n",
        "            trust_remote_code=True,\n",
        "            use_safetensors=True,\n",
        "            device_map=\"auto\",\n",
        "            quantization_config=qc,\n",
        "            torch_dtype=torch.float\n",
        "        )\n",
        "        self.model = self.model.eval()\n",
        "        self.log(\"Decoder model loaded successfully!\")\n",
        "\n",
        "    def log(self, message):\n",
        "        \"\"\"Log message to both console and file.\"\"\"\n",
        "        print(f\"[DECODER] {message}\")\n",
        "        self.log_file.write(f\"{message}\\n\")\n",
        "        self.log_file.flush()\n",
        "\n",
        "    def build_pair_prompt(self, obj_a, obj_b):\n",
        "        \"\"\"Build prompt for relationship generation between two objects.\"\"\"\n",
        "        return f\"\"\"You are a vision-language model reasoning about an indoor scene.\n",
        "\n",
        "Object A:\n",
        "- name: {obj_a['name']}\n",
        "- tag: {obj_a['tag']}\n",
        "- description: {obj_a['caption'] or \"no extra description\"}\n",
        "- center (x, y, z): {obj_a['center']}\n",
        "- extent (dx, dy, dz): {obj_a['extent']}\n",
        "\n",
        "Object B:\n",
        "- name: {obj_b['name']}\n",
        "- tag: {obj_b['tag']}\n",
        "- description: {obj_b['caption'] or \"no extra description\"}\n",
        "- center (x, y, z): {obj_b['center']}\n",
        "- extent (dx, dy, dz): {obj_b['extent']}\n",
        "\n",
        "Question:\n",
        "What is the relationship between Object A and Object B in the scene?\n",
        "Please generate a single, natural language caption describing their relationship,\n",
        "focusing on spatial or functional relationships. Be concise.\n",
        "\"\"\"\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate_caption(self, prompt):\n",
        "        \"\"\"Generate relationship caption using decoder-only mode.\"\"\"\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
        "\n",
        "        out = self.model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=MAX_NEW_TOKENS,\n",
        "            do_sample=False,\n",
        "            temperature=TEMPERATURE,\n",
        "            pad_token_id=self.tokenizer.eos_token_id,\n",
        "        )\n",
        "\n",
        "        generated = out[0]\n",
        "        full_text = self.tokenizer.decode(generated, skip_special_tokens=True)\n",
        "\n",
        "        # Strip original prompt\n",
        "        if full_text.startswith(prompt):\n",
        "            return full_text[len(prompt):].strip()\n",
        "        return full_text.strip()\n",
        "\n",
        "    def extract_relationship(self, caption):\n",
        "        \"\"\"Extract relationship keyword from caption.\"\"\"\n",
        "        text = caption.lower()\n",
        "        for kw in RELATION_KEYWORDS:\n",
        "            if kw in text:\n",
        "                return kw\n",
        "        return \"related to\"\n",
        "\n",
        "    def process_objects(self, obj_json_path):\n",
        "        \"\"\"Process object JSON to generate relationships.\"\"\"\n",
        "        self.log(f\"\\nLoading objects from: {obj_json_path}\")\n",
        "\n",
        "        # Load objects\n",
        "        with open(obj_json_path, \"r\") as f:\n",
        "            obj_dict = json.load(f)\n",
        "\n",
        "        # Convert to list\n",
        "        objects = []\n",
        "        for key, info in obj_dict.items():\n",
        "            objects.append({\n",
        "                \"name\": key,\n",
        "                \"id\": info.get(\"id\"),\n",
        "                \"tag\": info.get(\"object_tag\", \"\"),\n",
        "                \"caption\": info.get(\"object_caption\", \"\"),\n",
        "                \"center\": info.get(\"bbox_center\", None),\n",
        "                \"extent\": info.get(\"bbox_extent\", None),\n",
        "            })\n",
        "\n",
        "        self.log(f\"Loaded {len(objects)} objects\")\n",
        "\n",
        "        # Generate relationships for all pairs\n",
        "        edges = {}\n",
        "        edge_id = 0\n",
        "\n",
        "        total_pairs = len(list(itertools.combinations(objects, 2)))\n",
        "        self.log(f\"Generating relationships for {total_pairs} object pairs...\")\n",
        "\n",
        "        for i, (obj_a, obj_b) in enumerate(itertools.combinations(objects, 2)):\n",
        "            if (i + 1) % 10 == 0:\n",
        "                self.log(f\"Progress: {i+1}/{total_pairs} pairs processed\")\n",
        "\n",
        "            prompt = self.build_pair_prompt(obj_a, obj_b)\n",
        "            caption = self.generate_caption(prompt)\n",
        "            relationship = self.extract_relationship(caption)\n",
        "\n",
        "            edge_key = f\"edge_{edge_id}\"\n",
        "            edges[edge_key] = {\n",
        "                \"edge_id\": edge_id,\n",
        "                \"edge_description\": caption,\n",
        "                \"num_detections\": 1,\n",
        "                \"object_1_id\": obj_a[\"id\"],\n",
        "                \"object_1_tag\": obj_a[\"tag\"],\n",
        "                \"object_2_id\": obj_b[\"id\"],\n",
        "                \"object_2_tag\": obj_b[\"tag\"],\n",
        "                \"relationship\": relationship,\n",
        "            }\n",
        "\n",
        "            self.log(f\"{edge_key}: {obj_a['tag']} <-> {obj_b['tag']} | {relationship}\")\n",
        "            edge_id += 1\n",
        "\n",
        "        # Save edges\n",
        "        output_path = self.log_dir / \"decoder_outputs\" / \"relationships.json\"\n",
        "        with open(output_path, \"w\") as f:\n",
        "            json.dump(edges, f, indent=2)\n",
        "\n",
        "        self.log(f\"\\n✓ Decoder complete: {len(edges)} relationships saved to {output_path}\")\n",
        "        return edges\n",
        "\n",
        "    def close(self):\n",
        "        \"\"\"Close log file.\"\"\"\n",
        "        self.log_file.close()\n",
        "\n",
        "# ============================================================================\n",
        "# MAIN PIPELINE\n",
        "# ============================================================================\n",
        "\n",
        "def main():\n",
        "    \"\"\"Run complete pipeline: Encoder → Decoder.\"\"\"\n",
        "    print(\"=\" * 80)\n",
        "    print(\"SCENE GRAPH PIPELINE\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Setup logging\n",
        "    log_dir = setup_log_directory()\n",
        "    print(f\"\\nLog directory: {log_dir}\")\n",
        "\n",
        "    if True:\n",
        "      # Check if input directory exists\n",
        "      if not Path(IMAGE_INPUT_DIR).exists():\n",
        "          print(f\"\\n✗ Error: Input directory not found: {IMAGE_INPUT_DIR}\")\n",
        "          print(\"Please create the directory and add your images (jpg/png)\")\n",
        "          return\n",
        "\n",
        "      # === PHASE 1: ENCODER (Object Detection) ===\n",
        "      print(\"\\n\" + \"=\" * 80)\n",
        "      print(\"PHASE 1: OBJECT DETECTION (ENCODER)\")\n",
        "      print(\"=\" * 80)\n",
        "\n",
        "      encoder = ObjectEncoder(MODEL_NAME, log_dir)\n",
        "      encoder_results = encoder.process_directory(IMAGE_INPUT_DIR)\n",
        "      encoder.close()\n",
        "\n",
        "      if not encoder_results:\n",
        "          print(\"\\n✗ No images were successfully processed by encoder\")\n",
        "          return\n",
        "\n",
        "    if True:\n",
        "          # === PHASE 2: DECODER (Relationship Generation) ===\n",
        "          print(\"\\n\" + \"=\" * 80)\n",
        "          print(\"PHASE 2: RELATIONSHIP GENERATION (DECODER)\")\n",
        "          print(\"=\" * 80)\n",
        "\n",
        "          # For now, use the provided object JSON\n",
        "          # In a full pipeline, you'd parse encoder outputs to create this JSON\n",
        "\n",
        "          obj_json_path = Path(IMAGE_INPUT_DIR / \"detection_result.json\")\n",
        "\n",
        "          if not obj_json_path.exists():\n",
        "              print(f\"\\n✗ Error: Object JSON not found: {obj_json_path}\")\n",
        "              print(\"Please ensure obj_json_r_mapping_stride10.json is in the working directory\")\n",
        "              return\n",
        "\n",
        "          decoder = RelationshipDecoder(MODEL_NAME, log_dir)\n",
        "          relationships = decoder.process_objects(obj_json_path)\n",
        "          decoder.close()\n",
        "\n",
        "    # === COMPLETE ===\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"PIPELINE COMPLETE!\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"\\nAll outputs saved to: {log_dir}\")\n",
        "    print(f\"  - Encoder outputs: {log_dir / 'encoder_outputs'}\")\n",
        "    print(f\"  - Decoder outputs: {log_dir / 'decoder_outputs'}\")\n",
        "    print(f\"  - Logs: encoder_log.txt, decoder_log.txt\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 778
        },
        "id": "3BZQWdfqk9vT",
        "outputId": "82a7e963-4863-41fd-c621-15308cae83a1"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "SCENE GRAPH PIPELINE\n",
            "================================================================================\n",
            "\n",
            "Log directory: frame00180_test/logs/log_25121001\n",
            "\n",
            "================================================================================\n",
            "PHASE 1: OBJECT DETECTION (ENCODER)\n",
            "================================================================================\n",
            "[ENCODER] Initializing Object Encoder...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using a model of type deepseek_vl_v2 to instantiate a model of type DeepseekOCR. This is not supported for all configurations of models and can yield errors.\n",
            "Some weights of DeepseekOCRForCausalLM were not initialized from the model checkpoint at deepseek-ai/DeepSeek-OCR and are newly initialized: ['model.vision_model.embeddings.position_ids']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ENCODER] Encoder model loaded successfully!\n",
            "[ENCODER] \n",
            "Found 1 images to process\n",
            "[ENCODER] \n",
            "Processing: frame00180_test/frame000180.jpg\n",
            "[ENCODER] ✓ Successfully processed frame000180.jpg\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'parsed' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1918922853.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-1918922853.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m       \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mObjectEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_NAME\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m       \u001b[0mencoder_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_directory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIMAGE_INPUT_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m       \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1918922853.py\u001b[0m in \u001b[0;36mprocess_directory\u001b[0;34m(self, image_dir)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0mresult_json_file\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mimage_dir\u001b[0m\u001b[0;34m/\u001b[0m \u001b[0;34m\"object_result.json\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_json_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m           \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n✓ Encoder complete: {len(results)}/{len(image_files)} images processed\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'parsed' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is an instance run."
      ],
      "metadata": {
        "id": "nhir35TBdr6b"
      }
    }
  ]
}